{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# TASK 1: DATASET UNDERSTANDING\n",
        "# Titanic Dataset | One-Shot Program\n",
        "# ================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Task 1: Understanding Dataset & Data Types Started\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. CREATE PROJECT STRUCTURE\n",
        "# -------------------------------\n",
        "folders = [\"data\", \"outputs\", \"report\"]\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"üìÅ Project folders created\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. LOAD DATASET\n",
        "# -------------------------------\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.to_csv(\"data/titanic.csv\", index=False)\n",
        "print(\"üìä Titanic dataset loaded and saved\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. HEAD & TAIL RECORDS\n",
        "# -------------------------------\n",
        "head_tail = pd.concat([df.head(), df.tail()])\n",
        "head_tail.to_csv(\"outputs/head_tail.csv\", index=False)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. DATASET INFO\n",
        "# -------------------------------\n",
        "from io import StringIO\n",
        "buffer = StringIO()\n",
        "df.info(buf=buffer)\n",
        "\n",
        "with open(\"outputs/dataset_info.txt\", \"w\") as f:\n",
        "    f.write(buffer.getvalue())\n",
        "\n",
        "# -------------------------------\n",
        "# 5. STATISTICAL SUMMARY\n",
        "# -------------------------------\n",
        "describe_df = df.describe(include=\"all\")\n",
        "describe_df.to_csv(\"outputs/describe.csv\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. FEATURE TYPE IDENTIFICATION\n",
        "# -------------------------------\n",
        "numerical = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "binary = [col for col in df.columns if df[col].nunique() == 2]\n",
        "ordinal = [\"Pclass\"]\n",
        "\n",
        "# Save feature classification\n",
        "with open(\"outputs/feature_types.txt\", \"w\") as f:\n",
        "    f.write(f\"Numerical Features:\\n{numerical}\\n\\n\")\n",
        "    f.write(f\"Categorical Features:\\n{categorical}\\n\\n\")\n",
        "    f.write(f\"Binary Features:\\n{binary}\\n\\n\")\n",
        "    f.write(f\"Ordinal Features:\\n{ordinal}\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. UNIQUE VALUES IN CATEGORICAL\n",
        "# -------------------------------\n",
        "with open(\"outputs/categorical_unique_values.txt\", \"w\") as f:\n",
        "    for col in categorical:\n",
        "        f.write(f\"{col}:\\n{df[col].unique()}\\n\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. TARGET & ML FEATURES\n",
        "# -------------------------------\n",
        "target = \"Survived\"\n",
        "features = df.drop(columns=[target]).columns.tolist()\n",
        "\n",
        "# -------------------------------\n",
        "# 9. DATASET SIZE & QUALITY\n",
        "# -------------------------------\n",
        "rows, cols = df.shape\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "ml_readiness_report = f\"\"\"\n",
        "DATASET SIZE\n",
        "------------\n",
        "Rows: {rows}\n",
        "Columns: {cols}\n",
        "\n",
        "TARGET VARIABLE\n",
        "---------------\n",
        "{target}\n",
        "\n",
        "INPUT FEATURES\n",
        "--------------\n",
        "{features}\n",
        "\n",
        "MISSING VALUES\n",
        "--------------\n",
        "{missing_values}\n",
        "\n",
        "ML SUITABILITY\n",
        "--------------\n",
        "- Supervised Classification Dataset\n",
        "- Missing value handling required\n",
        "- Categorical encoding needed\n",
        "- Suitable for ML models after preprocessing\n",
        "\"\"\"\n",
        "\n",
        "with open(\"outputs/ml_readiness.txt\", \"w\") as f:\n",
        "    f.write(ml_readiness_report)\n",
        "\n",
        "# -------------------------------\n",
        "# 10. DATASET ANALYSIS REPORT\n",
        "# -------------------------------\n",
        "report_text = \"\"\"\n",
        "# Dataset Analysis Report ‚Äì Titanic Dataset\n",
        "\n",
        "## Dataset Overview\n",
        "The Titanic dataset consists of passenger details such as age, gender, ticket class, fare, and survival outcome.\n",
        "\n",
        "## Feature Classification\n",
        "- Numerical: Age, Fare, SibSp, Parch\n",
        "- Categorical: Name, Sex, Ticket, Cabin, Embarked\n",
        "- Binary: Survived, Sex\n",
        "- Ordinal: Pclass\n",
        "\n",
        "## Target Variable\n",
        "Survived (Binary Classification)\n",
        "\n",
        "## Data Quality Issues\n",
        "- Missing values in Age, Cabin, and Embarked\n",
        "- Cabin column has high missing percentage\n",
        "- Class imbalance in survival data\n",
        "\n",
        "## Machine Learning Readiness\n",
        "The dataset is suitable for supervised ML tasks after preprocessing steps such as:\n",
        "- Handling missing values\n",
        "- Encoding categorical variables\n",
        "- Feature scaling if needed\n",
        "\n",
        "## Conclusion\n",
        "This dataset is ideal for understanding data types, feature roles, and ML preparation.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"report/Dataset_Analysis_Report.md\", \"w\") as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "# -------------------------------\n",
        "# 11. README FILE\n",
        "# -------------------------------\n",
        "readme_text = \"\"\"\n",
        "# Task 1 ‚Äì Understanding Dataset & Data Types\n",
        "\n",
        "## Objective\n",
        "Analyze dataset structure, data types, feature roles, and ML readiness.\n",
        "\n",
        "## Dataset\n",
        "Titanic Dataset\n",
        "\n",
        "## Tools Used\n",
        "- Python\n",
        "- Pandas\n",
        "- NumPy\n",
        "- Google Colab\n",
        "\n",
        "## Deliverables\n",
        "- Jupyter Notebook\n",
        "- Dataset Analysis Report\n",
        "- Saved outputs (CSV & TXT files)\n",
        "\n",
        "## Key Learning\n",
        "Understanding dataset structure is the foundation of Machine Learning.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "# -------------------------------\n",
        "# FINAL MESSAGE\n",
        "# -------------------------------\n",
        "print(\"‚úÖ TASK COMPLETED SUCCESSFULLY\")\n",
        "print(\"üìÇ Outputs saved automatically\")\n",
        "print(\"üìÑ Report & README generated\")\n",
        "print(\"üöÄ Ready for GitHub push\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaNkkqpHwylz",
        "outputId": "daa57779-353f-4aa3-b6df-3cd62eaec8fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Task 1: Understanding Dataset & Data Types Started\n",
            "üìÅ Project folders created\n",
            "üìä Titanic dataset loaded and saved\n",
            "‚úÖ TASK COMPLETED SUCCESSFULLY\n",
            "üìÇ Outputs saved automatically\n",
            "üìÑ Report & README generated\n",
            "üöÄ Ready for GitHub push\n"
          ]
        }
      ]
    }
  ]
}